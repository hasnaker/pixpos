apiVersion: batch/v1
kind: Job
metadata:
  name: migrate-images-to-s3
  namespace: pixpos
spec:
  ttlSecondsAfterFinished: 300
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: migrate
        image: node:20-alpine
        command: ["/bin/sh", "-c"]
        args:
        - |
          apk add --no-cache curl
          npm install pg @aws-sdk/client-s3
          cat > migrate.js << 'SCRIPT'
          const { Client } = require('pg');
          const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
          const crypto = require('crypto');

          const S3_BUCKET = 'pixpos-assets-986906625644';
          const S3_REGION = 'eu-central-1';
          const CLOUDFRONT_DOMAIN = 'dh8ksnk9abt8f.cloudfront.net';

          const dbConfig = {
            host: process.env.DB_HOST,
            port: 5432,
            database: process.env.DB_NAME,
            user: process.env.DB_USER,
            password: process.env.DB_PASSWORD,
            ssl: { rejectUnauthorized: false }
          };

          const s3Client = new S3Client({ region: S3_REGION });

          async function migrateImages() {
            const client = new Client(dbConfig);
            
            try {
              await client.connect();
              console.log('âœ… DB connected');

              const result = await client.query(`
                SELECT id, name, image_url 
                FROM products 
                WHERE image_url IS NOT NULL 
                  AND image_url LIKE 'data:image%'
              `);

              console.log(`ğŸ“¦ Found ${result.rows.length} products with base64 images`);

              let migrated = 0;
              let failed = 0;

              for (const product of result.rows) {
                try {
                  const base64Data = product.image_url;
                  const matches = base64Data.match(/^data:image\/(\w+);base64,(.+)$/);
                  if (!matches) {
                    console.log(`âš ï¸ Invalid base64: ${product.name}`);
                    failed++;
                    continue;
                  }

                  const [, imageType, base64Content] = matches;
                  const buffer = Buffer.from(base64Content, 'base64');
                  const hash = crypto.createHash('md5').update(product.id).digest('hex').slice(0, 8);
                  const filename = `products/${hash}-${product.id}.${imageType}`;

                  await s3Client.send(new PutObjectCommand({
                    Bucket: S3_BUCKET,
                    Key: filename,
                    Body: buffer,
                    ContentType: `image/${imageType}`,
                    CacheControl: 'max-age=31536000'
                  }));

                  const cdnUrl = `https://${CLOUDFRONT_DOMAIN}/${filename}`;
                  await client.query('UPDATE products SET image_url = $1 WHERE id = $2', [cdnUrl, product.id]);

                  console.log(`âœ… ${product.name}`);
                  migrated++;
                } catch (err) {
                  console.error(`âŒ ${product.name}:`, err.message);
                  failed++;
                }
              }

              console.log(`\nğŸ“Š Done: ${migrated} migrated, ${failed} failed`);
            } catch (err) {
              console.error('âŒ Error:', err);
              process.exit(1);
            } finally {
              await client.end();
            }
          }

          migrateImages();
          SCRIPT
          node migrate.js
        env:
        - name: DB_HOST
          valueFrom:
            secretKeyRef:
              name: pixpos-secrets
              key: DB_HOST
        - name: DB_NAME
          valueFrom:
            secretKeyRef:
              name: pixpos-secrets
              key: DB_DATABASE
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: pixpos-secrets
              key: DB_USERNAME
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: pixpos-secrets
              key: DB_PASSWORD
        - name: AWS_REGION
          value: "eu-central-1"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
